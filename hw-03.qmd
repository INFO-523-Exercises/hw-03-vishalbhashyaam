---
title: "hw-03.qmd"
author: "VISHAL BHASHYAAM"
format: html
editor: visual
---

# Classification: Basic Concepts and Techniques

## Install packages

Install the required packages used in the chapter:

```{r}

if (!require(pacman))
  install.packages("pacman")

pacman::p_load(tidyverse ,rpart, rpart.plot, caret , lattice, FSelector,
               sampling,pROC)

# I am not installing ml bench because I am using different dataset which does not require mlbench.
```

## Spam E-mail Dataset

I am using the spam e-mail dataset which either requires the tidytuesdayR package or we can read it directly from the github.

The E-mail Spam dataset conatins 4601 rows of information mostly numerical except the last column.

The Spam E-mail Dataset contains 7 columns (crl.tot , dollar , bang , money , n000 , make , yesno) in this the first 6 columns represent the feature vector **x** and the last column is the class label **y.** We will convert the dataframe into tidyverse tibble else a normal table.

```{r}
# Reading the dataset directly from github

spam <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-15/spam.csv')
spamCsv <- spam
```

```{r}
spamCsv |>
  head() |>
  formattable::formattable()
```

```{r}
library(tidyverse)
as_tibble(spamCsv)
```

```{r}
spam |>
  summary() 
  
```

Converting yes/no values into factors(nominal), used to build models.

```{r}
spamCsv <- spamCsv |>
  mutate(across(where(is.logical), factor, levels = c("y", "n"))) |>
  mutate(across(where(is.character), factor))
```

```{r}
summary(spamCsv)
```

## Decision Trees

Recursive Partitioning uses the Gini index to make splitting decisions and early stopping (pre-pruning)

```{r}
library(rpart)
```

## Creating tree with default setting (uses pre-pruning)

```{r}
tree_default <-spamCsv |>
  rpart(yesno ~ ., data = _)

tree_default
```

-   The formula models the **`yesno`** variable by all other features represented by `.`

-   The class variable needs a factor or rpart will create a regression tree instead of a decision tree.

## Plotting

```{r}
library(rpart.plot)
rpart.plot(tree_default, extra =2)
```

-   This plots the decision created by the formula

## Creating a Full Tree

```{r}
tree_full <- spamCsv |> 
  rpart(yesno~. , data = _, 
        control = rpart.control(minsplit = 2, cp = 0))
rpart.plot(tree_full, extra = 2, 
           roundint=FALSE,
            box.palette = list("Gy", "Gn", "Bu", "Bn", 
                               "Or", "Rd", "Pu")) # specify 7 colors
```

-   The above plot is not clear because there is overplotting in the graph. So this part of full_tree is not posible to visualize for this dataset.

```{r}
tree_full
```

## Training error on tre with pre-pruning

```{r}
predict(tree_default,spamCsv) |> head()
```

```{r}
pred <- predict(tree_default, spamCsv, type="class")
head(pred)
```

```{r}
confusion_table <- with(spamCsv,table(yesno,pred))
confusion_table
```

-   We can see on the Confusion Matrix the correct number of true positive prediction and true negative predictions.

```{r}
correct  <- confusion_table |> diag() |> sum()
correct
```

-   Number of true predictions

```{r}
error  <- confusion_table |> sum() - correct
error
```

-   umber of false predictions

```{r}
accuracy <- correct / (correct + error)
accuracy
```

-   Formula for Accuracy

## Using a function for accuracy

```{r}
accuracy <- function(truth, prediction) {
    tbl <- table(truth, prediction)
    sum(diag(tbl))/sum(tbl)
}

accuracy(spamCsv |> pull(yesno), pred)
```

Training error of the full tree

```{r}
accuracy(spamCsv |> pull(yesno), predict(tree_full,spamCsv, type="class"))
```

Get a confusion table with more statistics (using caret)

```{r}
library(caret)
confusionMatrix(data = pred, 
                reference = spamCsv |> pull(yesno))
```

```{r}
spamCsv
```

## Making prediction for New Data

Making a new set of data, to create a new prediction

```{r}
my_spamCsv <- tibble(crl.tot = 23,
dollar=0.52,
bang=0.526,
money=0.15,
n000=0.00,
make=0.01,
yesno=NA)
```

Setting columns to be factors like in the training data

```{r}
my_spamCsv<-my_spamCsv |>
  mutate(across(where(is.logical),factor,levels =c("y","n")))
my_spamCsv
```

```{r}
predict(tree_default, my_spamCsv, type = "class")
```

## Model Evaluation with Caret

This package makes preparing training sets , building classification and regression model and evaluation simple .

```{r}
library(caret)
```

```{r}
set.seed(2000)
```

## Holding out Test data

Test data will not be used to build the data, therefore spilling the data into train and test, where training data will be used to build the model

```{r}
inTrain <- createDataPartition(y= spamCsv$yesno, p =.8, list= FALSE)
spamCsv_train <- spamCsv |> slice(inTrain)
```

```{r}
spamCsv_test <- spamCsv |> slice(-inTrain)
```

## Learn a Model and Tune Hyperparameters on the Training Data

For `rpart` train tries to tune the `cp` parameter using accuracy to chose the best model. minsplit is set to 2, but it can be set to higher values oif the data is abundant.

```{r}
fit <- spamCsv |>
  train(yesno~.,
        data=_ ,
        method = "rpart",
        control = rpart.control(minsplit = 2),
        trControl = trainControl(method = "cv", number = 10),
        tuneLength = 5)

fit
```

-   The Accuracy of the model is decent and Kappa value almost similar to the accuracy. Kappa values is usually used between 2 or more classes, since our model has two classes, we say that kappa value is also excellent.

A model using the best tuning parameters and using all the data supplied to `train()` is available as `fit$finalModel` .

```{r}
rpart.plot(fit$finalModel, extra =2 ,
           box.palette = list("Gy", "Gn", "Bu", "Bn", "Or", "Rd", "Pu"))
```

```{r}
varImp(fit)
```

Here is the variable importance without completing splits.

```{r}
imp <- varImp(fit, compete = FALSE)
imp
```

```{r}
ggplot(imp)
```

-   We can see from this plot that crt.tot, bang and dollar contributes in giving the prediction for a class than the other feature which has 0 importance or we can say it is entirely useless

## Testing: Confusion Matrix and Confidence Interval for Accuracy

Using the best model on the test data

```{r}
pred <- predict(fit, newdata= spamCsv_test)
pred
```

`carret` confusionMatrix calculates the accuracy, confidence intervals, kappa and other evaluation metrics. we need to use the test data to find the generallized error.

```{r}
confusionMatrix(data = pred,
                ref = spamCsv_test |> pull(yesno))
```

## Model Comparison

```{r}
train_index <- createFolds(spamCsv_train$yesno, k=10)
```

Building models

```{r}
rpartFit <- spamCsv_train |> 
  train(yesno ~ .,
        data = _,
        method = "rpart",
        tuneLength = 10,
        trControl = trainControl(method = "cv", indexOut = train_index)
  )
```

```{r}
knnFit <- spamCsv_train |> 
  train(yesno ~ .,
        data = _,
        method = "knn",
        preProcess = "scale",
          tuneLength = 10,
          trControl = trainControl(method = "cv", indexOut = train_index)
  )
```

Comparing accuracy over all folds:

```{r}
resamps <- resamples(list(
  CART = rpartFit,
  kNearestNeighbors = knnFit
))

summary(resamps)
```

-   We can see that K-Nearest Neighbor and CART performs really close in terms of accuracy and kappa.

```{r}
library(lattice)
bwplot(resamps, layout = c(3,1))
```

-   Overall we can see that K- Nearest Neighbor has a better accuracy and CART has a better Kappa value.

```{r}
difs <- diff(resamps)
difs
```

```{r}
summary(difs)
```

-   K Nearest seems to perform better but statistically classifiers do not perform differently.

## Feature Selection and Feature Preparation

Decision trees selesct the features for splitting implicitly, but we can also select the features

Installing the dependencies for `FSelector`

```{r}
if(!require(rJava))
  install.packages("rJava",type = "source")
if(!require(RWeka))
  install.packages("RWeka")

```

```{r}
Sys.setenv(JAVA_HOME='~/Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home')
```

```{r}
library(FSelector)
```

## Univariate Feature Importance Score

Shows the measure how each variable is associated with the class variable. For discrete features chi-square statistic is used.

```{r}
weights <-spamCsv |> 
  chi.squared(yesno ~ ., data = _) |>
  as_tibble(rownames = "feature") |>
  arrange(desc(attr_importance))

weights
```

Plotting importance in descending order (using `reorder` to order factor levels used by `ggplot`)

```{r}
ggplot(weights,
       aes(x = attr_importance, y = reorder(feature, attr_importance))) +
  geom_bar(stat = "identity") + 
  xlab("Important Score") +
  ylab("Feature")
```

-   Bang has the most reliability with the class variables.

Getting the best 4 features:

```{r}
subset <- cutoff.k(weights |>
                     column_to_rownames("feature"),4)

subset 
```

Using the best 4 features to build a model (`FSelector` provides `as.simple.formula`)

```{r}
f<- as.simple.formula(subset, "yesno")
f
```

```{r}
m <- spamCsv_train |> rpart(f, data= _)
rpart.plot(m, extra = 2, roundint = FALSE)
```

```{r}
spamCsv |>
  gain.ratio(yesno~. , data= _) |>
  as_tibble(rownames = "feature") |>
  arrange(desc(attr_importance))
```

## Feature Subset Selection

```{r}
spamCsv |>
  cfs(yesno ~ ., data= _)
```

Black-box feature selection uses an evaluator function (the black box) to calculate a score to be maximized. First, we define an evaluation function that builds a model given a subset of features and calculates a quality score. We use here the average for 5 bootstrap samples (`method = "cv"`Â can also be used instead), no tuning (to be faster), and the average accuracy as the score.

```{r}
evaluator <- function(subset) {
  model <- spamCsv_train |> 
    train(as.simple.formula(subset, "yesno"),
          data = _,
          method = "rpart",
          trControl = trainControl(method = "boot", number = 5),
          tuneLength = 0)
  results <- model$resample$Accuracy
  cat("Trying features:", paste(subset, collapse = " + "), "\n")
  m <- mean(results)
  cat("Accuracy:", round(m, 2), "\n\n")
  m
}
```

Start with all feature (but not the class variable `type`)

```{r}
features <- spamCsv_train |> colnames() |> setdiff("type") 
```

There are other greedy search strategies available but takes a lot of computational time

```{r}
##subset <- backward.search(features, evaluator)
##subset <- forward.search(features, evaluator)
##subset <- best.first.search(features, evaluator)
##subset <- hill.climbing.search(features, evaluator)
##subset
```

## Using Dummy Variable as Factors

Using dummy variable is not possible, because I need to create it, and this dataset contains only categorical dataset which is the prediction for this dataset. I cannot convert any column in as.factor because each value is unique and random values (numerical). Just using money column to implement this technique.

```{r, warning=FALSE}
tree_predator <- spamCsv_train |>
  rpart(money ~ yesno, data= _)

rpart.plot(tree_predator,roundint = FALSE)
```

This code below doesn't make sense because "money" does not a have categorical value and `n` and `y` we cant predict anything with numerical value. just implementing the code for reference.

```{r}
spamCsv_train_dummy <- as_tibble(class2ind(spamCsv_train$yesno)) |> 
  mutate(across(everything(), as.factor)) |>
  add_column(money = spamCsv_train$money)
spamCsv_train_dummy 
```

```{r}
tree_predator <- spamCsv_train_dummy |> 
  rpart(money ~ ., 
        data = _,
        control = rpart.control(minsplit = 2, cp = 0.01))
rpart.plot(tree_predator, roundint = FALSE)
```

```{r}
fit <- spamCsv_train |> 
  train(money ~ ., 
        data = _, 
        method = "rpart",
        control = rpart.control(minsplit = 2),
        tuneGrid = data.frame(cp = 0.01))
fit
```

```{r}
rpart.plot(fit$finalModel)
```

## Class Imbalance

Classifiers have a hard time to learn from data where we have much more observations for one class (called the majority class). This is called the class imbalance problem

```{r}
library(rpart)
library(rpart.plot)
```

Class distribution

```{r}
ggplot(spamCsv, aes(y = yesno)) + geom_bar()
```

-   We need to create the classes "Spam" and "Not_Spam" to make it into binary classification problem,

-   This dataset is already a binary classification problem

```{r}
spamCsv_yes <- spamCsv |>
  mutate(yesno = factor(spamCsv$yesno == "y",
                       levels = c(TRUE,FALSE),
                       labels = c("Spam","Not_Spam")))
```

```{r}
summary(spamCsv_yes)
```

```{r}
ggplot(spamCsv_yes, aes(y=yesno)) + geom_bar()
```

Creating test and training data. Using a 50/50 split to make sure that the test set has some samples of the yes and no classes in them

```{r}
set.seed(1234)
nTrain <- createDataPartition(y = spamCsv_yes$yesno, p = .5, list = FALSE)
training_yes <-spamCsv_yes |> slice(inTrain)
testing_yes <- spamCsv_yes |> slice(-inTrain)
```

## Option 1: Use the Data as is and hope for the best

```{r}
fit <- training_yes |>
  train(yesno~.,
        data= _,
        method = "rpart",
        trControl = trainControl(method = "cv"))
```

```{r}
fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

The tree does not predict everything as spam instead there is a condition where it has spam and not_spam.

```{r}
confusionMatrix(data = predict(fit, testing_yes),
                ref = testing_yes$yesno, positive = "Spam")
```

-   Accuracy is decent and kappa aslo has manageable value, the no-information rate is lower than the accuracy this means that model is good. since we are dealing iwth imbalance , we have good score of sensitivity, the chance to identify positive examples.

## Option 2: Balance data with Resampling

We use stratified sampling with replacement (to oversample the minority/positive class). You could also use SMOTE (in packageÂ **DMwR**) or other sampling strategies (e.g., from packageÂ **unbalanced**). We use 50+50 observations here (**Note:**Â many samples will be chosen several times).

```{r}
library(sampling)
set.seed(1000) # for repeatability

id <- strata(training_yes, stratanames = "yesno", size = c(50, 50), method = "srswr")
training_yes_balanced <- training_yes |> 
  slice(id$ID_unit)
table(training_yes_balanced$yesno)
```

```{r}
fit <- training_yes_balanced |> 
  train(yesno ~ .,
        data = _,
        method = "rpart",
        trControl = trainControl(method = "cv"),
        control = rpart.control(minsplit = 5))

fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

Checking on the unbalanced testing data

```{r}
confusionMatrix(data = predict(fit, testing_yes),
                ref  =testing_yes$yesno, positive = "Spam")
```

-   With re sampling the accuracy has decreased and the value of kappa also decreased with the decrease in sensitivity, with imbalanced dataset we want the sensitivity to increase, therefore this method is not suitable compared to the one we did before.

There is a tradeoff between sensitivity and specificity (how many of the identified animals are really reptiles) The tradeoff can be controlled using the sample proportions. We can sample more Spam to increase sensitivity at the cost of lower specificity.

```{r}
id <- strata(training_yes, stratanames = "yesno", size = c(50, 100), method = "srswr")
training_yes_balanced <- training_yes |> 
  slice(id$ID_unit)
table(training_yes_balanced$yesno)
```

```{r}
fit <- training_yes_balanced |> 
  train(yesno ~ .,
        data = _,
        method = "rpart",
        trControl = trainControl(method = "cv"),
        control = rpart.control(minsplit = 5))

confusionMatrix(data = predict(fit, testing_yes),
                ref = testing_yes$yesno, positive = "Spam")
```

-   We can see that Sensitivity, Accuracy and the kappa has increased with a small reduction in specificity .

## Option 3 : Build a larger tree and use Predicted Probabilities

Increase in complexity and require less data, tries to improve accuracy .

```{r}
fit <- training_yes |> 
  train(yesno ~ .,
        data = _,
        method = "rpart",
        tuneLength = 10,
        trControl = trainControl(method = "cv",
        classProbs = TRUE,  ## necessary for predict with type="prob"
        summaryFunction=twoClassSummary),  ## necessary for ROC
        metric = "ROC",
        control = rpart.control(minsplit = 3))
```

```{r}
fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

```{r}
confusionMatrix(data = predict(fit, testing_yes),
                ref = testing_yes$yesno, positive = "Spam")
```

```{r}
prob <- predict(fit, testing_yes, type = "prob")
tail(prob)
```

```{r}
pred <- as.factor(ifelse(prob[,"Spam"]>=0.01, "Spam", "Not_Spam"))

confusionMatrix(data = pred,
                ref = testing_yes$yesno, positive = "Spam")
```

-   The Accuracy is very low, model is very bad. This maybe because the data is very large, so the complexity is also very larger compared, This plays a major role in assessing the accuracy.

## Plot the ROC curve

Since it is a binary classification problem, the area under the curve represents a single number for how well the classifier works (Closer to one is better)

```{r}
library("pROC")
r <- roc(testing_yes$yesno == "Spam", prob[,"Spam"])
```

```{r}
r
```

```{r}
ggroc(r) + geom_abline(intercept = 1, slope = 1, color = "darkgrey")
```

-   The line has a trend starts to get closer to 1 and at the end touches 1.

## Option 4: Use a Cost - Sensitive Classifier

The implementation of CART in `rpart` can use a cost matrix for making splitting decisions (as parameter `loss`). The matrix has the form

TP FP FN TN

TP and TN have to be 0. We make FN very expensive (100).

```{r}
cost <- matrix(c(
  0, 1,
  100, 0
), byrow = TRUE, nrow = 2)
cost
```

```{r}
fit <- training_yes |> 
  train(yesno ~ .,
        data = _,
        method = "rpart",
        parms = list(loss = cost),
        trControl = trainControl(method = "cv"))
```

```{r}
fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

```{r}
confusionMatrix(data = predict(fit, testing_yes),
                ref = testing_yes$yesno, positive = "Spam")
```

-   The accuracy of the model is not good , it is bad. The no-information rate is lower than the accuracy , kappa has a very bad value, The sensitivity and specificity trade off is also very bad. (True positive examples is very low i this method)

### Question:

Based on the frequency of certain keywords and symbols in an email, can we classify the email as 'spam' or 'not spam'?

Yes, according to the data analysis and model analysis completed above , we can say that we can classify the eamil as "Spam" and "Not_spam" based on the repetition of certain keywords and symbols

# Classification: Alternative Techniques

## Install packages

```{r}
if(!require(pacman))
  install.packages("pacman")

pacman::p_load(
  C50,                # C5.0 Decision Trees and Rule-Based Models
  caret,              # Classification and Regression Training
  e1071,              # Misc Functions of the Department of Statistics (e1071), TU Wien
  keras,              # R Interface to 'Keras'
  kernlab,            # Kernel-Based Machine Learning Lab
  lattice,            # Trellis Graphics for R
  MASS,               # Support Functions and Datasets for Venables and Ripley's MASS
  mlbench,            # Machine Learning Benchmark Problems
  nnet,               # Feedforward Neural Networks and Multinomial Log-Linear Models
  palmerpenguins,     # Palmer Archipelago (Antarctica) Penguin Data
  party,              # A Laboratory for Recursive Partytioning
  partykit,           # A Toolkit for Recursive Partytioning
  randomForest,       # Breiman and Cutler's Random Forests for Classification and Regression
  rpart,              # Recursive partitioning models
  RWeka,              # R/Weka Interface
  scales,             # Scale Functions for Visualization
  tidymodels,         # Tidy machine learning framework
  tidyverse,          # Tidy data wrangling and visualization
  xgboost             # Extreme Gradient Boosting
)
```

Show fewer Digits

```{r}
options(digits = 3)
```

# Introduction

## Training and Testing data

## Spam E-mail Dataset

I am using the spam e-mail dataset which either requires the tidytuesdayR package or we can read it directly from the github.

The E-mail Spam dataset conatins 4601 rows of information mostly numerical except the last column.

The Spam E-mail Dataset contains 7 columns (crl.tot , dollar , bang , money , n000 , make , yesno) in this the first 6 columns represent the feature vector **x** and the last column is the class label **y.** We will convert the dataframe into tidyverse tibble else a normal table.

```{r}
# We will use the email spam dataset as used earlier

SpamCsv <- spam
SpamCsv |> glimpse()
```

Multicore support does not work with rJava and RWeka

```{r}
##library(doMC, quietly = TRUE)
##registerDoMC(cores = 4)
##getDoParWorkers()
```

Testing data will not be used to build the model, will be used after building the model. Using 80% for training.

```{r}
set.seed(123) # for reproducibility
inTrain <- createDataPartition(y = SpamCsv$yesno, p=.8)[[1]]
SpamCsv_train <- dplyr::slice(SpamCsv,inTrain)
SpamCsv_test <- dplyr::slice(SpamCsv,-inTrain)
```

## Fitting different Classification Models to the Training Data

Creating a fixed sampling scheme(10-folds) , so that we can compare the fitted models later.

```{r}
train_index <- createFolds(SpamCsv_train$yesno, k=10)
```

## Conditional Inference tree (Decision Tree)

```{r}
ctreeFit <- SpamCsv |> train (yesno ~ ., 
                              method = "ctree",
                              data =_,
                               tuneLength = 5,
                              trControl = trainControl(method = "cv", indexOut = train_index))

ctreeFit
```

-   As we can see that accuracy for the model is high and Kappa value is also high

-   Since it is fitted on train data we will not know how it works on new data (Test data)

The Plot looks like a mess because there is a lot of features and data involved

```{r}
plot(ctreeFit$finalModel)
```

-   The best parameters tuned for the model is used in the above figure.

-   This is the result of overplotting.

## C 4.5 Decision Tree

```{r}
C45Fit <- SpamCsv_train |> train(yesno ~.,
                                 method = "J48",
                                 data = _,
                                 tuneLength = 5,
                                 trControl = trainControl(method = "cv", indexOut = train_index))
```

```{r}
C45Fit
```

-   C = 0.378 , M=1 ,Accuracy = 0.891 ,Kappa = 0.764

-   The accuracy for this model is again high and the Kappa is also relativly high and performs better than Conditional Inference tree.

```{r}
C45Fit$finalModel
```

-   The J48 pruned tree is shown above, there are 117 brains and 59 main nodes to the tree.

## K-Nearest Neighbors

```{r}
knnFit <- SpamCsv_train |> train(yesno ~ .,
  method = "knn",
  data = _,
  preProcess = "scale",
    tuneLength = 5,
  tuneGrid=data.frame(k = 1:10),
    trControl = trainControl(method = "cv", indexOut = train_index))
knnFit
```

-   The K- nearest Neighbor has better accuracy and kappa score than the decision tree.

-   it is the best model based on the training data , and might be overfitting, because it might perform bad on the test data

```{r}
knnFit$finalModel
```

-   The above out come distribution shows the prediction of class "y" and "n"

## PART (Rule-based classifier)

```{r}
rulesFit <- SpamCsv_train |> train(yesno ~ .,
  method = "PART",
  data = _,
  tuneLength = 5,
  trControl = trainControl(method = "cv", indexOut = train_index))
rulesFit
```

-   The above inference for PART gives a high accuracy and kappa score but is bad compared to the knn - classifier.

-   It might not be overfitting on the data and might perform well on the testing data.

```{r}
rulesFit$finalModel
```

-   The above inference shows the rules about the algorithm works on this training data.

## Linear Support Vector Machines

```{r}
svmFit <- SpamCsv_train |> train(yesno ~.,
  method = "svmLinear",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index))
svmFit
```

```{r}
svmFit$finalModel
```

-   Compared to knn the best classifier for this data until now, the accuracy is moderate, not good nor bad, the kappa score is very low compared to the knn classifier, the kappa score indicates the agreement between imbalanced categories in the dataset in terms to calculate the accuracy.

-   This kappa score means there is moderate agreement that the class variable is somewhat distributed in the dataset

## Random Forest

```{r}
randomForestFit <- SpamCsv_train |> train(yesno ~ .,
  method = "rf",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index))
randomForestFit
```

```{r}
randomForestFit$finalModel
```

-   The accuracy is better than the knn classifier, which makes it more prone to overfitting, the accuracy is very high and the kappa value means that there is near perfect agreement of the target variable distribution

## Gradient Boosted Decision Trees (xgboost)

```{r}
xgboostFit <- SpamCsv_train |> train(yesno ~ .,
  method = "xgbTree",
  data = _,
  tuneLength = 5,
  trControl = trainControl(method = "cv", indexOut = train_index),
  tuneGrid = expand.grid(
    nrounds = 20,
    max_depth = 3,
    colsample_bytree = .6,
    eta = 0.1,
    gamma=0,
    min_child_weight = 1,
    subsample = .5
  ))
xgboostFit
```

-   Accuracy of the model is comparatively low than the best classifier (random forest) until now.

-   The model is mediocre and does not perform well as the other models we have seen until now.

-   The reason I am just reffering any model as not good compared to the best model is because the we are only inferring on the training data, so i compare them to the best accuracy and kappa score.

```{r}
xgboostFit$finalModel
```

## Artificial Neural Networks

```{r}
nnetFit <- SpamCsv |> train(yesno ~ .,
                            method = "nnet",
                            data = _,
                             tuneLength = 5,
                            trControl = trainControl(method = "cv", indexOut = train_index),trace = FALSE)
nnetFit
```

-   The Accuracy for neural network is also mediocre when compared to the best model random forest classifier and the kappa score is also in moderate agreement.

```{r}
nnetFit$finalModel
```

## Comparing Models

Collecting the performance metrics from the models trained on the same data.

```{r}
resamps <- resamples(list(
  ctree = ctreeFit,
  C45 = C45Fit,
  SVM = svmFit,
  KNN = knnFit,
  rules = rulesFit,
  randomForest = randomForestFit,
  xgboost = xgboostFit,
  NeuralNet = nnetFit
    ))
resamps
```

```{r}
summary(resamps)
```

-   We can clearly see the Random Forest is the best model for classification, next to it is the K- Nearest Neighbor, all other models perform bad when compared to these models in terms of accuracy.

-   Kappa score is relatively close for knn and random forest, but still random forest has the better scores and this means it is in near perfect agreement.

```{r}
library(lattice)
bwplot(resamps, layout = c(3,1))
```

The Box - plot shows that random forest performed better in all ten folds and knn is the closest to it.

This means that the inference on the testing data might be similar for the both, not much of a difference in the predictions.

```{r}
difs <- diff(resamps)
difs 
```

```{r}
summary(difs)
```

-   This shows that there is a huge difference in the model, all model perform decent, but ctree in the first row shows has a negative value, it can rejected since it has \<0.05 value by null hypothesis.

## Applying the chosen model to the Test data

Most of the model do well on the train data, here we choose random forest model, since it has performed better than any other on the train data and hope it does good on the testing data as well.

```{r}
pr <- predict(randomForestFit, SpamCsv_test)

pr


```

```{r,warning=FALSE}
confusionMatrix(pr, reference = spamCsv_test$yesno)

```

-   The accuracy on the best model from training data which is random forest classifier performs good on the testing data as well.

-   This means it does not overfit on the testing data.

-   Maybe if we tweak the hyperparameters of the random forest model a bit, it might perform even better, or randomize the data for training.

-   We can also choose a another model like knn to see the inference.

## Comparing Decision Boundaries of Popular Classification Techniques.

```{r}
library(scales)
library(tidyverse)
library(ggplot2)
library(caret)

decisionplot <- function(model, data, class_var, 
  predict_type = c("class", "prob"), resolution = 3 * 72) {
  # resolution is set to 72 dpi if the image is rendered  3 inches wide. 
  
  y <- data |> pull(class_var)
  x <- data |> dplyr::select(-all_of(class_var))
  
  # resubstitution accuracy
  prediction <- predict(model, x, type = predict_type[1])
  # LDA returns a list
  if(is.list(prediction)) prediction <- prediction$class
  prediction <- factor(prediction, levels = levels(y))
  
  cm <- confusionMatrix(data = prediction, 
                        reference = y)
  acc <- cm$overall["Accuracy"]
  
  # evaluate model on a grid
  r <- sapply(x[, 1:2], range, na.rm = TRUE)
  xs <- seq(r[1,1], r[2,1], length.out = resolution)
  ys <- seq(r[1,2], r[2,2], length.out = resolution)
  g <- cbind(rep(xs, each = resolution), rep(ys, time = resolution))
  colnames(g) <- colnames(r)
  g <- as_tibble(g)
  
  ### guess how to get class labels from predict
  ### (unfortunately not very consistent between models)
  cl <- predict(model, g, type = predict_type[1])
  
  # LDA returns a list
  prob <- NULL
  if(is.list(cl)) { 
    prob <- cl$posterior
    cl <- cl$class
  } else
    if(!is.na(predict_type[2]))
      try(prob <- predict(model, g, type = predict_type[2]))
  
  # we visualize the difference in probability/score between the 
  # winning class and the second best class.
  # don't use probability if predict for the classifier does not support it.
  max_prob <- 1
  if(!is.null(prob))
    try({
      max_prob <- t(apply(prob, MARGIN = 1, sort, decreasing = TRUE))
      max_prob <- max_prob[,1] - max_prob[,2]
    }, silent = TRUE) 
  
  cl <- factor(cl, levels = levels(y))
  
  g <- g |> add_column(prediction = cl, probability = max_prob)
  
  ggplot(g, mapping = aes(
    x = .data[[colnames(g)[1]]], y = .data[[colnames(g)[2]]])) +
    geom_raster(mapping = aes(fill = prediction, alpha = probability)) +
    geom_contour(mapping = aes(z = as.numeric(prediction)), 
      bins = length(levels(cl)), linewidth = .5, color = "black") +
    geom_point(data = data, mapping =  aes(
      x = .data[[colnames(data)[1]]], 
      y = .data[[colnames(data)[2]]],
      shape = .data[[class_var]]), alpha = .7) + 
    scale_alpha_continuous(range = c(0,1), limits = c(0,1), guide = "none") +  
    labs(subtitle = paste("Training accuracy:", round(acc, 2))) +
     theme_minimal(base_size = 14)
}
```

## Using Penguins Dataset

For easier visualization, we use two dimensions of the penguins dataset. Contour lines visualize the density like mountains on a map

```{r}
set.seed(1000)
data("penguins")
penguins<- as_tibble(penguins) |>
  drop_na()
### Three classes
x<- penguins |> dplyr::select(bill_length_mm,bill_depth_mm,species)
x
```

```{r}
# Using geom_jitter instead of geom_point since there is some overplotting
ggplot(x, aes(x = bill_length_mm, y = bill_depth_mm, fill = species)) +  
  stat_density_2d(geom = "polygon", aes(alpha = after_stat(level))) +
  geom_jitter() +
  theme_minimal(base_size = 14) +
  labs(x = "Bill length (mm)",
       y = "Bill depth (mm)",
       fill = "Species",
       alpha = "Density")
```

-   In the above plot we can see that there 3 classes and there are some points which is outside those 3 classes =, we can call them outlier as they wont affect the prediction in any way. We can remove them using some techniques ised in statistics.

## K-Nearest Neighbors Classifier

```{r}
model <- x |> caret::knn3(species ~ ., data= _,k = 1)
decisionplot(model, x, class_var = "species") + 
  labs(title = "kNN (1 neighbor)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   at k = 1 the training accuracy is one, this means it has predicted the classes perfectly, but we can see that the areas are over lapping each other

```{r}
model <- x |> caret::knn3(species ~ ., data = _, k = 3)
decisionplot(model, x, class_var = "species") + 
  labs(title = "kNN (3 neighbor)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   The white areas the common points or the spots are in the same place .

```{r}
model <- x |> caret::knn3(species ~ ., data = _, k = 9)
decisionplot(model, x, class_var = "species") + 
  labs(title = "kNN (9 neighbor)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   As k = 9 which is increased the boundary get smooth and we get a clear picture of the boundaries, it fits better than k = 1

## Naive bayes Classifier

```{r}
model <- x |> e1071::naiveBayes(species ~ ., data = _)
decisionplot(model, x, class_var = "species", 
             predict_type = c("class", "raw")) + 
  labs(title = "Naive Bayes",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction") 
```

-   Naive bayes relies on probability, it has also done a good job in diving the boundaries for different classes.

## Linear Discriminant Analysis

```{r}
model <- x |> MASS::lda(species ~ ., data = _)
decisionplot(model, x, class_var = "species") + 
  labs(title = "LDA",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

## Multinomial Logistic Regression

```{r}
model <- x |> nnet::multinom(species ~., data = _)
decisionplot(model, x, class_var = "species") + 
  labs(title = "Multinomial Logistic Regression",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")


```

-   We can the prediction of the boundaries for multinomial is different than the LDA and has more accuracy, but not that significant.

## Decision trees

```{r}
model <- x |> rpart::rpart(species ~ ., data = _)
decisionplot(model, x, class_var = "species") + 
  labs(title = "CART",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

```{r}
model <- x |> rpart::rpart(species ~ ., data = _,
  control = rpart.control(cp = 0.001, minsplit = 1))
decisionplot(model, x, class_var = "species") + 
  labs(title = "CART (overfitting)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   Training accuracy of overfitting is higher because an higher order complexity is introduced to over fit the data.

-   THe pink space int the plot shows the common point for 3 classes.

```{r}
model <- x |> C50::C5.0(species ~ ., data = _)
decisionplot(model, x, class_var = "species") + 
  labs(title = "C5.0",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   The lighter green is still chinstrap but the prediction is not that confident because it is near the boundary.

-   The accuray is very high

## Random Forest

```{r}
model <- x |> randomForest::randomForest(species ~ ., data = _)
decisionplot(model, x, class_var = "species") + 
  labs(title = "Random Forest",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   Random forest has performed good with the dataset, 100% it has perfect predictions.

-    By looking at the boundary, how curvy it is, we can see it has better boundaries for the classes.

## SVM

```{r}
model <- x |> e1071::svm(species ~ ., data = _, kernel = "linear")
decisionplot(model, x, class_var = "species") + 
  labs(title = "SVM (linear kernel)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   Linear kernel has straight lines as the boundaries with no curve .

```{r}
model <- x |> e1071::svm(species ~ ., data = _, kernel = "radial")
decisionplot(model, x, class_var = "species") + 
  labs(title = "SVM (radial kernel)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   Radial kernel has some sort curve in them, it leans towards the point in the boundaries to assume its shape.

```{r}
model <- x |> e1071::svm(species ~ ., data = _, kernel = "polynomial")
decisionplot(model, x, class_var = "species") + 
  labs(title = "SVM (polynomial kernel)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   It increases its complexity of the model to bring a better prediction, but this has lower accuracy compared to the other SVM's.

```{r}
model <- x |> e1071::svm(species ~ ., data = _, kernel = "sigmoid")
decisionplot(model, x, class_var = "species") + 
  labs(title = "SVM (sigmoid kernel)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   The Sigmoid kernel divides it s boundaries similar to the sin graph.

## Single Layered Feed Forward Neural Network

```{r}
model <-x |> nnet::nnet(species ~ ., data = _, size = 1, trace = FALSE)
decisionplot(model, x, class_var  = "species", 
  predict_type = c("class", "raw")) + 
  labs(title = "NN (1 neuron)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   Performs good has high accuracy, but the boundaries can be improved by increasing the number of neurons

```{r}
model <-x |> nnet::nnet(species ~ ., data = _, size = 2, trace = FALSE)
decisionplot(model, x, class_var  = "species", 
  predict_type = c("class", "raw")) + 
  labs(title = "NN (2 neurons)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   Increasing the neuron increased the accuracy , the boundaries look like a linear line with some deviation.

```{r}
model <-x |> nnet::nnet(species ~ ., data = _, size = 4, trace = FALSE)
decisionplot(model, x, class_var  = "species", 
  predict_type = c("class", "raw")) + 
  labs(title = "NN (4 neurons)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   again Increasing the neuron again increased the accuracy, has defined a better boundary for the classes

```{r}
model <-x |> nnet::nnet(species ~ ., data = _, size = 10, trace = FALSE)
decisionplot(model, x, class_var  = "species", 
  predict_type = c("class", "raw")) + 
  labs(title = "NN (10 neurons)",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       shape = "Species",
       fill = "Prediction")
```

-   10 neurons defines even better boundary showing common points(white area).

-   Comparatively better performance.

## Circle Dataset

This dataset is not linearly separable

```{r}
set.seed(1000)

x <- mlbench::mlbench.circle(500)
###x <- mlbench::mlbench.cassini(500)
###x <- mlbench::mlbench.spirals(500, sd = .1)
###x <- mlbench::mlbench.smiley(500)
x <- cbind(as.data.frame(x$x), factor(x$classes))
colnames(x) <- c("x", "y", "class")
x <- as_tibble(x)
x
```

```{r}
ggplot(x, aes(x = x, y = y, color = class)) + 
  geom_point() +
  theme_minimal(base_size = 14)
```

## K-Nearest Classifier

```{r}
model <- x |> caret::knn3(class ~ ., data = _, k = 1)
decisionplot(model, x, class_var = "class") + 
  labs(title = "kNN (1 neighbor)",
       shape = "Class",
       fill = "Prediction")
```

-   Again proves to be a better classifier compared to other models, But the k = 1 has no definition in the boundary for the points lying on the boundary.

```{r}
model <- x |> caret::knn3(class ~ ., data = _, k = 10)
decisionplot(model, x, class_var = "class") + 
  labs(title = "kNN (10 neighbor)",
       shape = "Class",
       fill = "Prediction")
```

-   Has a better boundary definition than the k = 1. Clearly has defined two boundaries

## Naive Bayes Classifier

```{r}
model <- x |> e1071::naiveBayes(class ~ ., data = _)
decisionplot(model, x, class_var = "class", 
  predict_type = c("class", "raw")) + 
  labs(title = "naive Bayes",
       shape = "Class",
       fill = "Prediction")
```

-   This model has low confidence level of predicting the class as we can see the faded red for the prediction of a class.

## Linear Discriminant Analysis (LDA)

```{r}
model <- x |> MASS::lda(class ~ ., data = _)
decisionplot(model, x, class_var = "class") + 
  labs(title = "LDA",
       shape = "Class",
       fill = "Prediction")
```

-   has a boundary line for the the classes, but as we can see again this model has no confidence in prediction as we can more white space than the predicition of classes

## Logistic Regression

Multinomial logistic regression is an extension of logistic regression to problems with more than two classes. It also tries to find a linear decision boundary.

```{r}
model <- x |> nnet::multinom(class ~., data = _)
decisionplot(model, x, class_var = "class") + 
  labs(title = "Multinomial Logistic Regression",
       shape = "Class",
       fill = "Prediction")
```

-   H class 2 is found abundant. but the predictions are wrong, the accuracy is very low, this is a circle dataset, but a linear line is separating as boundaries.

## Decision Trees

```{r}
model <- x |> rpart::rpart(class ~ ., data = _)
decisionplot(model, x, class_var = "class") + 
  labs(title = "CART",
       shape = "Class",
       fill = "Prediction")
```

-   Has a bit of less definition for the boundary as it is not smooth.

```{r}
model <- x |> rpart::rpart(class ~ ., data = _,
  control = rpart.control(cp = 0.001, minsplit = 1))
decisionplot(model, x, class_var = "class") + 
  labs(title = "CART (overfitting)",
       shape = "Class",
       fill = "Prediction")
```

-   Overfitting ensures it has a prediction, and the boundary has well defined shape for the points lying at the boundary line.

```{r}
model <- x |> C50::C5.0(class ~ ., data = _)
decisionplot(model, x, class_var = "class") + 
  labs(title = "C5.0",
       shape = "Class",
       fill = "Prediction")
```

-   CS 50 has a similar boundary to CART, but the prediction of classes vary.

## Random Forest

```{r}
library(randomForest)
model <- x |> randomForest(class ~ ., data = _)
decisionplot(model, x, class_var = "class") + 
  labs(title = "Random Forest",
       shape = "Class",
       fill = "Prediction")
```

-   Boundaries are well defined with white spaces indicating common points, with a high accuracy of 100!% perfect prediction of classes.

## SVM

```{r}
model <- x |> e1071::svm(class ~ ., data = _, kernel = "linear")
decisionplot(model, x, class_var = "class") + 
  labs(title = "SVM (linear kernel)",
       shape = "Class",
       fill = "Prediction")
```

-   The Linear kernel predicts every prediction as class 2. it is a bad model.

```{r}
model <- x |> e1071::svm(class ~ ., data = _, kernel = "radial")
decisionplot(model, x, class_var = "class") + 
  labs(title = "SVM (radial kernel)",
       shape = "Class",
       fill = "Prediction")
```

-   Compared to linear kernel radial kernel performs way better with a high accuracy.

```{r}
model <- x |> e1071::svm(class ~ ., data = _, kernel = "polynomial")
decisionplot(model, x, class_var = "class") + 
  labs(title = "SVM (polynomial kernel)",
       shape = "Class",
       fill = "Prediction")
```

-   The polynomial kernel perform bad on circle dataset, as it gives curves in the boundaries but not completes as a circle and so it has a low accuracy.

```{r}
model <- x |> e1071::svm(class ~ ., data = _, kernel = "sigmoid")
decisionplot(model, x, class_var = "class") + 
  labs(title = "SVM (sigmoid kernel)",
       shape = "Class",
       fill = "Prediction")
```

-   It is similar to the polynomial kernel, perform very bad on circle datasets.

## Single Layered Feed Forward Neural Network

```{r}
model <-x |> nnet::nnet(class ~ ., data = _, size = 1, trace = FALSE)
decisionplot(model, x, class_var = "class", 
  predict_type = c("class")) + 
  labs(title = "NN (1 neuron)",
       shape = "Class",
       fill = "Prediction")
```

-   It is circle dataset, but the model has predicted a linear line to seperate, which might be wrong and has a very low accuracy.

```{r}
model <-x |> nnet::nnet(class ~ ., data = _, size = 2, trace = FALSE)
decisionplot(model, x, class_var = "class", 
  predict_type = c("class")) + 
  labs(title = "NN (2 neurons)",
       shape = "Class",
       fill = "Prediction")
```

-   Similar to the 1NN it is performing bad but has improved

```{r}
model <-x |> nnet::nnet(class ~ ., data = _, size = 4, trace = FALSE)
decisionplot(model, x, class_var = "class", 
  predict_type = c("class")) + 
  labs(title = "NN (4 neurons)",
       shape = "Class",
       fill = "Prediction")
```

-   it has a better boundary definition compared to the 1 neuron neural network, and has almost perfect accuracy.

```{r}
model <-x |> nnet::nnet(class ~ ., data = _, size = 10, trace = FALSE)
decisionplot(model, x, class_var = "class", 
  predict_type = c("class")) + 
  labs(title = "NN (10 neurons)",
       shape = "Class",
       fill = "Prediction")
```

-   At the end we can see that increasing the number of neurons increases the definition of the boundaries. Performs better than NN 4.

We can see that Knn , Random Forest and 10 neuron Neural Network worked better with the circle dataset.
